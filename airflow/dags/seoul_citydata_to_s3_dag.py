from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.exceptions import AirflowFailException
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from common_utils import skip_at_kst_21, load_sql
from datetime import datetime, timedelta
from urllib.parse import quote
import requests
import json

# =============================
# DAG ì„¤ì •
# =============================
DAG_ID = "seoul_citydata_to_s3_postgres"
SCHEDULE = "*/30 * * * *"

AREAS = [
    "ëª…ë™ ê´€ê´‘íŠ¹êµ¬",
    "ê´‘í™”ë¬¸Â·ë•ìˆ˜ê¶",
    "ì–´ë¦°ì´ëŒ€ê³µì›",
    "ì—¬ì˜ë„",
    "ì„±ìˆ˜ì¹´íŽ˜ê±°ë¦¬",
    "í™ëŒ€ìž…êµ¬ì—­(2í˜¸ì„ )",
    "ê°•ë‚¨ì—­",
    "DMC(ë””ì§€í„¸ë¯¸ë””ì–´ì‹œí‹°)",
    "ê´‘ìž¥(ì „í†µ)ì‹œìž¥",
    "ì„œìš¸ì‹ë¬¼ì›Â·ë§ˆê³¡ë‚˜ë£¨ì—­",
]

BASE_URL = "http://openapi.seoul.go.kr:8088/{api_key}/json/citydata/1/5/{area}"

SCHEMA_NAME = "raw_data"
TABLE_NAME = f"{SCHEMA_NAME}.realtime_city_data"

default_args = {
    "owner": "data-eng",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

# =============================
# 1ï¸âƒ£ API í˜¸ì¶œ â†’ S3 ì—…ë¡œë“œ
# =============================
def fetch_and_upload(**context):
    api_key = Variable.get("seoul_api_key", default_var=None)
    if not api_key:
        raise AirflowFailException("âŒ Airflow Variable 'seoul_api_key'ê°€ ì—†ìŠµë‹ˆë‹¤")

    bucket = Variable.get("s3_bucket_name")
    s3_hook = S3Hook(aws_conn_id="conn_aws")

    execution_time = context["data_interval_start"]

    success = []
    failed = []

    for area in AREAS:
        encoded_area = quote(area, safe="")
        url = BASE_URL.format(api_key=api_key, area=encoded_area)

        try:
            res = requests.get(url, timeout=10)

            if res.status_code != 200:
                failed.append({"area": area, "reason": f"HTTP {res.status_code}"})
                continue

            try:
                data = res.json()
            except Exception:
                failed.append({
                    "area": area,
                    "reason": "JSON íŒŒì‹± ì‹¤íŒ¨",
                    "response_sample": res.text[:200]
                })
                continue

            if "CITYDATA" not in data or not data["CITYDATA"]:
                failed.append({
                    "area": area,
                    "reason": "CITYDATA ì—†ìŒ",
                    "response_keys": list(data.keys())
                })
                continue

            date_part = execution_time.strftime("%Y-%m-%d")
            time_part = execution_time.strftime("%Y%m%d%H%M")

            # âœ… ë„¤ê°€ ì›í•˜ëŠ” key í˜•ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            key = (
                f"{date_part}/city_data/"
                f"{time_part}-ì„œìš¸ì‹œ_ì‹¤ì‹œê°„_ë„ì‹œë°ì´í„°-{area}.json"
            )

            s3_hook.load_string(
                string_data=json.dumps(data, ensure_ascii=False),
                key=key,
                bucket_name=bucket,
                replace=True,
            )

            success.append(area)

        except Exception as e:
            failed.append({"area": area, "reason": str(e)})

    print("âœ… SUCCESS AREAS:", success)
    print("âŒ FAILED AREAS:", failed)

    if failed:
        raise AirflowFailException(f"âŒ ì¼ë¶€ ì§€ì—­ ìˆ˜ì§‘ ì‹¤íŒ¨: {failed}")

    return {"success": success}

# =============================
# 2ï¸âƒ£ PostgreSQL í…Œì´ë¸” ìƒì„±
# =============================
def create_table_if_not_exists():
    hook = PostgresHook(postgres_conn_id="conn_postgres")
    conn = hook.get_conn()
    cur = conn.cursor()

    print(f"ðŸ“Œ í…Œì´ë¸” ìƒì„± ì‹œë„: {TABLE_NAME}")

    create_query = load_sql(
        filename="create_realtime_city_data.sql",
        dag_file=__file__,
        TABLE_NAME=TABLE_NAME
    )

    cur.execute(create_query)
    conn.commit()

    cur.close()
    conn.close()


# =============================
# 3ï¸âƒ£ S3 â†’ PostgreSQL ì ìž¬
# =============================
def insert_s3_data_to_postgres(**context):
    ti = context["ti"]
    success_areas = ti.xcom_pull(
        task_ids="fetch_citydata_to_s3"
    )["success"]

    bucket = Variable.get("s3_bucket_name")
    s3_hook = S3Hook(aws_conn_id="conn_aws")

    hook = PostgresHook(postgres_conn_id="conn_postgres")
    conn = hook.get_conn()
    cur = conn.cursor()

    execution_time = context["data_interval_start"]

    for area in success_areas:
        date_part = execution_time.strftime("%Y-%m-%d")
        time_part = execution_time.strftime("%Y%m%d%H%M")

        key = (
            f"{date_part}/city_data/"
            f"{time_part}-ì„œìš¸ì‹œ_ì‹¤ì‹œê°„_ë„ì‹œë°ì´í„°-{area}.json"
        )

        data_str = s3_hook.read_key(key=key, bucket_name=bucket)
        data_json = json.loads(data_str)

        cur.execute(
            f"""
            INSERT INTO {TABLE_NAME} (area_name, data)
            VALUES (%s, %s)
            """,
            (area, json.dumps(data_json, ensure_ascii=False))
        )

    conn.commit()
    cur.close()
    conn.close()

# =============================
# DAG ì •ì˜
# =============================
with DAG(
    dag_id=DAG_ID,
    default_args=default_args,
    start_date=datetime(2025, 1, 1),
    schedule_interval=SCHEDULE,
    catchup=False,
    tags=["seoul", "citydata", "s3", "postgres"],
) as dag:

    fetch_citydata_to_s3 = PythonOperator(
        task_id="fetch_citydata_to_s3",
        python_callable=fetch_and_upload,
    )

    create_table = PythonOperator(
        task_id="create_table",
        python_callable=create_table_if_not_exists,
    )

    insert_to_postgres = PythonOperator(
        task_id="insert_to_postgres",
        python_callable=insert_s3_data_to_postgres,
    )

    fetch_citydata_to_s3 >> create_table >> insert_to_postgres
